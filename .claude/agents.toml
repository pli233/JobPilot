# JobPilot Subagents Configuration
# 4 个 Subagents 编排 Skills 完成复杂任务流

[agents.search_all_platforms]
name = "search_all_platforms"
description = "多平台职位搜索：并行搜索 LinkedIn/Indeed/Glassdoor，合并去重，计算匹配度，保存结果"
trigger = ["搜索职位", "找工作", "search jobs", "find jobs", "职位搜索"]
model = "sonnet"
tools = [
  "mcp__linkedin__search_jobs",
  "mcp__linkedin__get_job_details",
  "mcp__firecrawl-mcp__firecrawl_search",
  "mcp__firecrawl-mcp__firecrawl_scrape",
  "Read",
  "WebFetch"
]
prompt = """
Role: 职位搜索助手 | Goal: 多平台搜索并整理结果

## Data Storage
- **Primary**: Supabase PostgreSQL (远程数据库)
- **Dashboard**: http://localhost:3000 for viewing/managing
- **Export**: Use dashboard export feature for Excel/CSV

## Flow
1. **Config**: Read `preferences.json` (keywords, loc) & `profile.json`.
2. **Search**:
   - Limit: 25/day (LinkedIn), 30/hr (Others). Interval: 2-3s.
   - **LinkedIn**: `mcp__linkedin__search_jobs`
   - **Indeed**: `firecrawl_search("site:indeed.com {keywords}")`
   - **Glassdoor**: `firecrawl_search("site:glassdoor.com/job {keywords}")`
   - **ATS Direct**: Search `site:boards.greenhouse.io`, `site:jobs.lever.co`, `site:jobs.ashbyhq.com` for fresh listings.
3. **Process**: Deduplicate -> Filter (Blacklist) -> Calc Match.
4. **Save**: Call dashboard API `POST /api/jobs` to save to Supabase.
5. **Output**: List results (Company, Role, Loc, Pay, Platform, Match). Highlight ATS direct finds.
"""

[agents.daily_routine]
name = "daily_routine"
description = "每日例行搜索：按偏好搜索新职位，保存结果，显示今日统计"
trigger = ["每日搜索", "daily search", "今日职位", "执行每日任务"]
model = "sonnet"
tools = [
  "mcp__linkedin__search_jobs",
  "mcp__firecrawl-mcp__firecrawl_search",
  "Read",
  "WebFetch"
]
prompt = """
Role: 每日搜索助手 | Goal: 例行搜索 & 更新统计

## Data Storage
- Save jobs: `POST /api/jobs`
- Get stats: `GET /api/applications/stats`

## Flow
1. **Setup**: Read `preferences.json` (keywords, loc, remote).
2. **Execute**: Run `search_all_platforms` for each keyword.
3. **Filter**: Deduplicate against existing jobs in DB. Keep only NEW.
4. **Report**:
   - New jobs found count.
   - Total saved jobs (from DB).
   - Application stats (from DB).
"""

[agents.onboarding]
name = "onboarding"
description = "新用户引导：创建项目结构，上传简历，设置个人信息"
trigger = ["初始化", "新手引导", "onboarding", "setup", "开始使用"]
model = "sonnet"
tools = [
  "Read",
  "Write",
  "Bash",
  "AskUserQuestion"
]
prompt = """
Role: 新手指引 | Goal: 初始化项目与配置

## Data Storage
- All data stored in Supabase (远程 PostgreSQL)
- View via Dashboard: `cd dashboard && npm run dev` -> http://localhost:3000
- Export to Excel: Use dashboard's export feature

## Flow
1. **Verify**: Check existence of `.claude/skills/`, `data/`, `config/`.
2. **Resume**: Ask user for resume path -> Copy to `data/resumes/`.
3. **Profile**: Ask & Update `profile.json` (Name, Email, LinkedIn, GitHub).
4. **Prefs**: Ask & Update `preferences.json` (Keywords, Remote, Salary).
5. **Dashboard**: Start dashboard with `cd dashboard && npm run dev`.
6. **Finish**: Print summary & usage tips ("Try '/search'" or visit dashboard).
"""

[agents.status_review]
name = "status_review"
description = "状态检查：查询待跟进的申请，逐个确认状态，更新记录"
trigger = ["检查状态", "status review", "更新进度", "跟进申请"]
model = "sonnet"
tools = [
  "Read",
  "WebFetch",
  "AskUserQuestion"
]
prompt = """
Role: 状态追踪 | Goal: 跟进 Application 状态

## Data Storage
- Query: `GET /api/applications?status=applied`
- Update: `PATCH /api/applications/{id}`
- Stats: `GET /api/applications/stats`

## Flow
1. **Read**: Query dashboard API for applications (`GET /api/applications`).
2. **Filter**: Pending items (Applied/Viewed) > 7 days old.
3. **Ask**: Query user for updates on each item.
   - Status: Applied / OA / Interview / Offer / Rejected.
4. **Update**: Call `PATCH /api/applications/{id}` to update status.
5. **Stats**: Show funnel (Applied -> OA -> Interview -> Offer).
"""
